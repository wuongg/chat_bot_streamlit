import faiss
import numpy as np
import os
import pickle
from sentence_transformers import SentenceTransformer
from docx import Document

# Load mÃ´ hÃ¬nh embedding
model = SentenceTransformer("all-MiniLM-L6-v2")

# ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a file .docx
folder_path = "ta"

# ÄÆ°á»ng dáº«n file lÆ°u dá»¯ liá»‡u
index_file = "faiss_index.bin"
embeddings_file = "embeddings.npy"
documents_file = "documents.pkl"

# HÃ m Ä‘á»c ná»™i dung tá»« file .docx
def read_docx(file_path):
    doc = Document(file_path)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

# Äá»c vÃ  xá»­ lÃ½ toÃ n bá»™ file trong thÆ° má»¥c
def process_folder(folder_path):
    documents = []
    filenames = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".docx"):
            file_path = os.path.join(folder_path, filename)
            content = read_docx(file_path)
            if content:
                documents.extend(content)  # Chia nhá» tá»«ng Ä‘oáº¡n vÄƒn
                filenames.extend([filename] * len(content))
    return documents, filenames

# Kiá»ƒm tra náº¿u Ä‘Ã£ cÃ³ dá»¯ liá»‡u lÆ°u sáºµn
if os.path.exists(index_file) and os.path.exists(embeddings_file) and os.path.exists(documents_file):
    print("ğŸ”„ Äang táº£i FAISS index vÃ  dá»¯ liá»‡u Ä‘Ã£ lÆ°u...")
    
    # Load FAISS index
    index = faiss.read_index(index_file)
    
    # Load embeddings
    embeddings = np.load(embeddings_file)
    
    # Load danh sÃ¡ch tÃ i liá»‡u
    with open(documents_file, "rb") as f:
        documents, filenames = pickle.load(f)
    
    print("âœ… Táº£i dá»¯ liá»‡u hoÃ n táº¥t!")
else:
    print("ğŸ“‚ Äang xá»­ lÃ½ tÃ i liá»‡u vÃ  táº¡o FAISS index...")

    # Äá»c dá»¯ liá»‡u tá»« thÆ° má»¥c
    documents, filenames = process_folder(folder_path)

    # MÃ£ hÃ³a vÄƒn báº£n thÃ nh embeddings
    embeddings = np.array(model.encode(documents, batch_size=32, convert_to_numpy=True, normalize_embeddings=True), dtype=np.float32)

    # Thiáº¿t láº­p FAISS Index IVF + HNSW
    d = embeddings.shape[1]
    nlist = 100  # Sá»‘ lÆ°á»£ng cá»¥m trong IVF
    quantizer = faiss.IndexHNSWFlat(d, 32)  # HNSW vá»›i 32 neighbors
    index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)

    # Huáº¥n luyá»‡n vÃ  thÃªm dá»¯ liá»‡u vÃ o index
    index.train(embeddings)
    index.add(embeddings)

    # Cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c báº±ng cÃ¡ch tÄƒng sá»‘ cá»¥m Ä‘Æ°á»£c duyá»‡t khi tÃ¬m kiáº¿m
    index.nprobe = 10  

    # LÆ°u FAISS index
    faiss.write_index(index, index_file)
    
    # LÆ°u embeddings
    np.save(embeddings_file, embeddings)
    
    # LÆ°u danh sÃ¡ch tÃ i liá»‡u
    with open(documents_file, "wb") as f:
        pickle.dump((documents, filenames), f)

    print("âœ… LÆ°u dá»¯ liá»‡u hoÃ n táº¥t!")

# HÃ m tÃ¬m kiáº¿m vÄƒn báº£n gáº§n nháº¥t
def retrieve_text_faiss(user_question, top_k=10):
    query_vector = np.array([model.encode(user_question, normalize_embeddings=True)], dtype=np.float32)
    distances, indices = index.search(query_vector, k=top_k)

    results = []
    for i, idx in enumerate(indices[0]):
        if idx < len(documents):  # Kiá»ƒm tra trÃ¡nh lá»—i index out of range
            score = 1 - distances[0][i]  # Chuyá»ƒn khoáº£ng cÃ¡ch L2 thÃ nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng
            results.append(f"[{filenames[idx]}] (Score: {score:.4f})\n{documents[idx]}")

    return "\n\n".join(results) if results else "KhÃ´ng tÃ¬m tháº¥y vÄƒn báº£n liÃªn quan."
